# PromptEngineering

<a href="https://discord.gg/bZ42ZFaSEm">
    <img src="https://img.shields.io/badge/Discord-Community-orange" alt="Community" />
</a>


## 1. 교과목 개요

* **과목명:** 프롬프트 엔지니어링
* **학점/시수:** 3학점 (이론 3)
* **담당교원:** 이영호 (국립목포대학교 컴퓨터학부)
* **과목 성격:** LLM 기반 실습 중심 교양/교직 과목

---

## 2. 교과목 목표

###  학습 목표

* LLM의 **동작 원리와 한계 이해**
* Zero/One/Few-shot, 역할 지시, CoT 등 **핵심 프롬프트 기법 적용**
* 모델 응답 **평가·검증 능력 함양**
* 윤리·보안 관점을 고려한 **안전한 AI 활용 능력 배양**

---

## 3. 주요 학습 내용

###  이론 기초

* AI/ML/DL/LLM 개념
* Transformer & Self-Attention
* GPT 발전 과정
* RLHF, 사전학습·미세튜닝
* LLM 한계 및 환각(Hallucination)

###  프롬프트 핵심 기법

* 명확성·맥락·형식 지정
* Zero/One/Few-shot
* 역할 지시(Role Prompt)
* Chain-of-Thought(CoT)
* Prompt Tuning
* RAG & ReAct
* Auto Prompting, ToT

###  평가·윤리

* 응답 품질 평가 (BLEU, ROUGE 등)
* 할루시네이션 검증
* 편향·유해성·저작권·프라이버시
* 안전 가드레일 설계

###  최신 동향

* Agentic AI
* 자율성과 인간 역할 변화

---

## 수업 방식

* 강의 중심 / 동영상 강의
* 실습 및 데모 포함
* 퀴즈 및 사례 분석 병행

---

##  성적 평가

| 항목     | 비율   |
| ------ | ---- |
| 출석     | 10%  |
| 중간     | 30%  |
| 기말     | 40%  |
| 과제     | 20%  |
| **총점** | 100% |

---

## 주차별 흐름 구조

1–3주: AI/LLM 이론 기초 \
4–8주: 프롬프트 핵심 기법 심화 \
9–10주: 고급 프롬프트 & RAG/ReAct \
11주: 응답 평가 \
12주: 윤리·법적 이슈 \
13주: 생성 AI 및 Agentic AI 

---


## 참고 영상
- [딥러닝이 주목받은 이유](https://youtu.be/fVIRvk0XGiM)
- [자연어 처리의 발전과 LLM의 등장](https://youtu.be/NCok3ZEK1Jk)
- [LLM의 학습방식, 사전학습과 미세조정](https://youtu.be/Ggq_Gq4MgnI)
- [LLM 핵심 원리와 양면성](https://youtu.be/lqJGoZeqB3A)
- [LLM의 자원 소비와 현실적인 비용 구조](https://youtu.be/HFavf41kEAk)
- [실전 데모형 프롬프트 예시](https://youtu.be/2u0wzVT9Z-M)
- 지도학습
- 비지도학습
- 강화학습
- 환각의 정의 유형 그리고 근본원인
- 환각완화를 위한 6가지 전략
- 프롬프트 정의
- 프롬프트 형식 요소
- 프롬프트 결과차이
- 프롬프트의 특징
- 나쁜 프롬프트의 특징과 좋은 프롬프트의 구성
- 프롬프트 개선 전략 및 실무 적용 방안
- 프롬프트 엔지니어링와 전통적 프로그래밍과 비교
- 산업 현장의 프롬프트 엔지니어링 활용 사례
- 효과적인 프롬프트 작성방법 핵심원칙 기억법
- 연쇄 추론 기법 CoT 개념
- 연쇄 추론 기법 CoT 응용
- 자가일관성 개념과 원리
- 자가일관성 효과 및 유의사항
- AI 에이전트의 시대
- 인공지능의 편향과 책임 그리고 저작권 이슈

## Papers

- **Prompt Engineering Techniques**:

  - [The Prompt Report: A Systematic Survey of Prompting Techniques](https://arxiv.org/abs/2406.06608) [2024] (Arxiv)
  - [Text Mining for Prompt Engineering: Text-Augmented Open Knowledge Graph Completion via PLMs](https://aclanthology.org/2023.findings-acl.709.pdf) [2023] (ACL)
  - [A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT](https://arxiv.org/abs/2302.11382) [2023] (Arxiv)
  - [Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery](https://arxiv.org/abs/2302.03668) [2023] (Arxiv)
  - [Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models](https://arxiv.org/abs/2302.00618) [2023] (Arxiv) 
  - [Progressive Prompts: Continual Learning for Language Models](https://arxiv.org/abs/2301.12314) [2023] (Arxiv) 
  - [Batch Prompting: Efficient Inference with LLM APIs](https://arxiv.org/abs/2301.08721) [2023] (Arxiv)
  - [Successive Prompting for Decompleting Complex Questions](https://arxiv.org/abs/2212.04092) [2022] (Arxiv) 
  - [Structured Prompting: Scaling In-Context Learning to 1,000 Examples](https://arxiv.org/abs/2212.06713) [2022] (Arxiv) 
  - [Large Language Models Are Human-Level Prompt Engineers](https://arxiv.org/abs/2211.01910) [2022] (Arxiv) 
  - [Ask Me Anything: A simple strategy for prompting language models](https://paperswithcode.com/paper/ask-me-anything-a-simple-strategy-for) [2022] (Arxiv) 
  - [Prompting GPT-3 To Be Reliable](https://arxiv.org/abs/2210.09150) [2022](Arxiv) 
  - [Decomposed Prompting: A Modular Approach for Solving Complex Tasks](https://arxiv.org/abs/2210.02406) [2022] (Arxiv) 
  - [PromptChainer: Chaining Large Language Model Prompts through Visual Programming](https://arxiv.org/abs/2203.06566) [2022] (Arxiv) 
  - [Investigating Prompt Engineering in Diffusion Models](https://arxiv.org/abs/2211.15462) [2022] (Arxiv) 
  - [Show Your Work: Scratchpads for Intermediate Computation with Language Models](https://arxiv.org/abs/2112.00114) [2021] (Arxiv) 
  - [Reframing Instructional Prompts to GPTk's Language](https://arxiv.org/abs/2109.07830) [2021] (Arxiv) 
  - [Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity](https://arxiv.org/abs/2104.08786) [2021] (Arxiv) 
  - [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691) [2021] (Arxiv) 
  - [Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm](https://arxiv.org/abs/2102.07350) [2021] (Arxiv) 
  - [Prefix-Tuning: Optimizing Continuous prompts for Generation](https://arxiv.org/abs/2101.00190) [2021] (Arxiv) 

- **Agent**:
  - [Intelligent AI Delegation](https://arxiv.org/pdf/2602.11865) [2026] (Arxiv) 

- **Reasoning and In-Context Learning**:

  - [Multimodal Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2302.00923) [2023] (Arxiv) 
  - [On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning](https://arxiv.org/abs/2212.08061) [2022] (Arxiv) 
  - [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629) [2022] (Arxiv) 
  - [Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought](https://arxiv.org/abs/2210.01240v3) [2022] (Arxiv) 
  - [On the Advance of Making Language Models Better Reasoners](https://arxiv.org/abs/2206.02336) [2022] (Arxiv) 
  - [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916) [2022] (Arxiv) 
  - [Reasoning Like Program Executors](https://arxiv.org/abs/2201.11473) [2022] (Arxiv)
  - [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171) [2022] (Arxiv) 
  - [Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://arxiv.org/abs/2202.12837) [2022] (Arxiv) 
  - [Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering](https://arxiv.org/abs/2209.09513v2) [2022] (Arxiv) 
  - [Chain of Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903) [2021] (Arxiv) 
  - [Generated Knowledge Prompting for Commonsense Reasoning](https://arxiv.org/abs/2110.08387) [2021] (Arxiv) 
  - [BERTese: Learning to Speak to BERT](https://aclanthology.org/2021.eacl-main.316) [2021] (Acl) 

- **Evaluating and Improving Language Models**:

  - [Large Language Models Can Be Easily Distracted by Irrelevant Context](https://arxiv.org/abs/2302.00093) [2023] (Arxiv) 
  - [Crawling the Internal Knowledge-Base of Language Models](https://arxiv.org/abs/2301.12810) [2023] (Arxiv) 
  - [Discovering Language Model Behaviors with Model-Written Evaluations](https://arxiv.org/abs/2212.09251) [2022] (Arxiv) 
  - [Calibrate Before Use: Improving Few-Shot Performance of Language Models](https://arxiv.org/abs/2102.09690) [2021] (Arxiv) 

- **Applications of Language Models**:

  - [Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves](https://arxiv.org/abs/2311.04205) [2023] (Arxiv)
  - [Prompting for Multimodal Hateful Meme Classification](https://arxiv.org/abs/2302.04156) [2023] (Arxiv) 
  - [PLACES: Prompting Language Models for Social Conversation Synthesis](https://arxiv.org/abs/2302.03269) [2023] (Arxiv) 
  - [Commonsense-Aware Prompting for Controllable Empathetic Dialogue Generation](https://arxiv.org/abs/2302.01441) [2023] (Arxiv) 
  - [PAL: Program-aided Language Models](https://arxiv.org/abs/2211.10435) [2023](Arxiv) 
  - [Legal Prompt Engineering for Multilingual Legal Judgement Prediction](https://arxiv.org/abs/2212.02199) [2023] (Arxiv) 
  - [Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language](https://arxiv.org/abs/2210.15157) [2022] (Arxiv) 
  - [Plot Writing From Scratch Pre-Trained Language Models](https://aclanthology.org/2022.inlg-main.5) [2022] (Acl) 
  - [AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts](https://arxiv.org/abs/2010.15980) [2020] (Arxiv) 

- **Threat Detection and Adversarial Examples**:

  - [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073) [2022] (Arxiv) 
  - [Ignore Previous Prompt: Attack Techniques For Language Models](https://arxiv.org/abs/2211.09527) [2022] (Arxiv) 
  - [Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods](https://arxiv.org/abs/2210.07321) [2022] (Arxiv) 
  - [Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples](https://arxiv.org/abs/2209.02128) [2022] (Arxiv) 
  - [Toxicity Detection with Generative Prompt-based Inference](https://arxiv.org/abs/2205.12390) [2022] (Arxiv) 
  - [How Can We Know What Language Models Know?](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00324/96460/How-Can-We-Know-What-Language-Models-Know) [2020] (Mit) 

- **Few-shot Learning and Performance Optimization**:

  - [Promptagator: Few-shot Dense Retrieval From 8 Examples](https://arxiv.org/abs/2209.11755) [2022] (Arxiv) 
  - [The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning](https://arxiv.org/abs/2205.03401) [2022] (Arxiv) 
  - [Making Pre-trained Language Models Better Few-shot Learners](https://aclanthology.org/2021.acl-long.295) [2021] (Acl) 
  - [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) [2020] (Arxiv) 

- **Text to Image Generation**:

  - [A Taxonomy of Prompt Modifiers for Text-To-Image Generation](https://arxiv.org/abs/2204.13988) [2022] (Arxiv) 
  - [Design Guidelines for Prompt Engineering Text-to-Image Generative Models](https://arxiv.org/abs/2109.06977) [2021] (Arxiv)
  - [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752) [2021] (Arxiv)
  - [DALL·E: Creating Images from Text](https://arxiv.org/abs/2102.12092) [2021] (Arxiv)

- **Text to Music/Sound Generation**:

  - [MusicLM: Generating Music From Text](https://arxiv.org/abs/2301.11325) [2023] (Arxiv) 
  - [ERNIE-Music: Text-to-Waveform Music Generation with Diffusion Models](https://arxiv.org/pdf/2302.04456) [2023] (Arxiv)
  - [Noise2Music: Text-conditioned Music Generation with Diffusion Models](https://arxiv.org/abs/2301.11325) [2023) (Arxiv)
  - [AudioLM: a Language Modeling Approach to Audio Generation](https://arxiv.org/pdf/2209.03143) [2023] (Arxiv)
  - [Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models](https://arxiv.org/pdf/2301.12661.pdf) [2023] (Arxiv)

- **Text to Video Generation**:

  - [Dreamix: Video Diffusion Models are General Video Editors](https://arxiv.org/pdf/2302.01329.pdf) [2023] (Arxiv) 
  - [Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation](https://arxiv.org/pdf/2212.11565.pdf) [2022] (Arxiv)
  - [Noise2Music: Text-conditioned Music Generation with Diffusion Models](https://arxiv.org/abs/2301.11325) [2023) (Arxiv)
  - [AudioLM: a Language Modeling Approach to Audio Generation](https://arxiv.org/pdf/2209.03143) [2023] (Arxiv)

- **Overviews**:

  - [Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?](https://arxiv.org/abs/2210.14699) [2022] (Arxiv) 

  
